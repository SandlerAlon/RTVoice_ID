# -*- coding: utf-8 -*-
# """main.ipynb
#
# Automatically generated by Colaboratory.
#
# Original file is located at
#     https://colab.research.google.com/drive/1Qw-gS5mxUPvWfrcn9YtqZ376yVDQ9OpC
# """

import pandas as pd
from pyspark import SparkContext
from pyspark.streaming import StreamingContext
from pyspark.sql import SparkSession
import numpy as np
from numpy.linalg import norm
import json
from pandas import json_normalize

# def main():
#     # : get session parameters - as list of jsons ?

#     # : load from ADX the session data - whats ADX ?
#     #   rdd = current session data
# '''
# 1)
# gets json needs to be formatted to df like -
# [[0,0,0,0,0,0],[0,1,2,3,45,5],[4,5,6,7,8]], index=['001','002','003']
# '''

data_txt = '{ "name":"John", "age":30, "city":"New York"}'
data_json = json.loads(data_txt)
print(json.dumps(data_json))


# sc = SparkContext.getOrCreate()
# df = pd.DataFrame([[0,0,0,0,0,0],[0,1,2,3,45,5],[4,5,6,7,8]], index=['001','002','003'])
# df_rd = sc.parallelize(df)
# # df_rd = SparkContext.fromDataFrame(df)
# print(df_rd.collect())
#
#
# #     # TODO: turn dataframe to rdd
# # here we will convert data frame to rdd -
# # df_rd=spark.createDataFrame(df,[list of params - field names]) - option 2
# # e.g - rdd = SparkContext.fromDataFrame(df) - option 3
#
# #     # TODO: listen to kafka producer
# # here i will triger the listen "while loop" for check if os is "on"=1
# # https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html - check this configuration in spark docs
# # Subscribe to 1 topic - option docs
# # df = spark \
# #   .readStream \
# #   .format("kafka") \
# #   .option("kafka.bootstrap.servers", "host1:port1,host2:port2") \
# #   .option("subscribe", "topic1") \
# #   .load()
# # df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
#
# # ##########################################################3
# # another option to connect as consumer
# # from pyspark.sql import SparkSession
# # spark = SparkSession \
# #         .builder \
# #         .appName("test") \
# #         .config("spark.sql.debug.maxToStringFields", "100") \
# #         .getOrCreate()
# # kafka_df = spark.readStream \
# #     .format("kafka") \
# #     .option("kafka.bootstrap.servers", "host1:port1,host2:port2") \
# #     .option("kafka.security.protocol", "SSL") \
# #     .option("failOnDataLoss", "false") \
# #     .option("subscribe", "topic1") \
# #     .option("includeHeaders", "true") \
# #     .option("startingOffsets", "latest") \
# #     .option("spark.streaming.kafka.maxRatePerPartition", "50") \
# #     .load()
#
# #     # TODO: get vector from kafka
#
# #     # TODO: calculate cosine simialrity with each row in rdd
# # ###################
#
# # # Spark process for real time voice identification
# # # Creation date 20.3.22
# # # Owner: Tzah
#
# # ###################
#
#
# # import pandas as pd
# # from pyspark import SparkContext
# # import numpy as np
# # from numpy.linalg import norm
#
# # def main():
# #     # : get session parameters
#
# #     # : load from ADX the session data
# #         # rdd = current session data

# #     df = pd.DataFrame([[0,0,0,0,0,0],[0,1,2,3,45,5],[4,5,6,7,8]], index=['001','002','003'])
# #     # TODO: turn dataframe to rdd
# #     # e.g - rdd = SparkContext.fromDataFrame(df)
#
# #     # TODO: listen to kafka consumer (liya)
#
# #     # TODO: get vector from kafka
data_txt = '{ "name":"John", "age":30, "city":"New York"}'
data_json = json.loads(data_txt)
print(json.dumps(data_json))
# #     # TODO: calculate cosine simialrity with each row in rdd
#
# #     # return closest vector's speaker id
# #     return df
#
#
# # if __name__=="__main__":
# #     print("Running spark proccess")
# #     print(main())
